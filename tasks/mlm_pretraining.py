"""
Pretraining a BERT-like model for masked language modeling.
"""