---
hf_user: martiwey
dataset_name: gh-java-methods-small-ood-train

# the strategy used to fine-tune the model: joint | naive | cumulative.
#   `joint`: fine-tuning on the whole dataset (i.e., offline).
#   `naive`: naive sequential fine-tuning.
#   `cumulative`: fine-tuning with access to all data from previous experience.
#   `si`: synaptic intelligence.
#   `ewc`: elastic weight consolidation.
#   `agem`: average gradient episodic memory
strategy: naive

# Synaptic intelligence HP
si_lambda: 10
si_eps: 0.1

# EWC HP
ewc_lambda: 0.3

# AGEM HP
agem_n_patterns_per_exp: 250
agem_sample_size: 256

train_batch_size: 6
valid_batch_size: 6
learning_rate: 5e-5
num_epochs_per_experience: 10
patience: 2   # early-stopping

# we filter out samples with ground-truth longer than `max_new_tokens`.
max_new_tokens: 30

# OOD domain datasets. The ordering defines the order in which the continual fine-tuning operates.
domains:
  - general
  - security
  - android
  - web
  - guava