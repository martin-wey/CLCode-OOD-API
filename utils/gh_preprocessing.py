"""
Preprocessing a dataset.
    - Deduplication using hash.
    - Removing samples with more than 250 lines of code.

Adapted from: https://huggingface.co/datasets/codeparrot/github-code/blob/main/github_preprocessing.py
"""

import hashlib
import os
import re
from argparse import Namespace

from datasets import load_dataset

# Settings
config = {
    'dataset_path': '../cl-code-apis/gh-java-data',
    'num_workers': 64,
    'line_max': 250,
    'repo_name': 'github-java-methods',
    'org': 'martiwey',
    'shard_size': 1000 << 20
}

args = Namespace(**config)

PATTERN = re.compile(r'\s+')


def hash_func(text):
    return hashlib.md5(re.sub(PATTERN, '', text).encode('utf-8')).hexdigest()


def get_hash(example):
    """Get hash of content field."""
    return {'hash': hash_func(example['source_code'])}


def line_stats(example):
    """Calculates the number of line of the method."""
    return {'lines': len(example['source_code'].splitlines())}


def check_uniques(example, uniques):
    """Check if current hash is still in set of unique hashes and remove if true."""
    if example['hash'] in uniques:
        uniques.remove(example['hash'])
        return True
    else:
        return False


def is_autogenerated(example, scan_width=5):
    """Check if file is autogenerated by looking for keywords in the first few lines of the file."""
    keywords = ['auto-generated', 'autogenerated', 'automatically generated']
    lines = example['source_code'].splitlines()
    for _, line in zip(range(scan_width), lines):
        for keyword in keywords:
            if keyword in line.lower():
                return {'autogenerated': True}
    else:
        return {'autogenerated': False}


def preprocess(example):
    """Chain all preprocessing steps into one function to not fill cache."""
    results = dict()
    results.update(get_hash(example))
    results.update(line_stats(example))
    return results


def filter(example, uniques, args):
    """Filter dataset with heuristics."""
    if not check_uniques(example, uniques):
        return False
    elif example['lines'] > args.line_max:
        return False
    else:
        return True


json_files = [os.path.join(args.dataset_path, pos_json) for pos_json in os.listdir(args.dataset_path)
              if pos_json.endswith('.json')]
print(f'Number of json files: {len(json_files)}')

# Load dataset
ds = load_dataset('json', data_files=json_files, chunksize=40 << 20)

# Run preprocessing
ds = ds.map(preprocess, num_proc=args.num_workers)
print(ds)

# Deduplicate hashes
uniques = set(ds['train']['hash'])
frac = len(uniques) / len(ds['train'])
print(f"Fraction of duplicates: {1 - frac:.2%}")

# Deduplicate data and apply heuristics
ds = ds.filter(filter, fn_kwargs={'uniques': uniques, 'args': args})
ds = ds.remove_columns(['lines', 'hash'])
print(f"Size of filtered dataset: {len(ds['train'])}")

# Save dataset in HF hub repo
ds.push_to_hub(f'{args.org}/{args.repo_name}', max_shard_size=args.shard_size)
